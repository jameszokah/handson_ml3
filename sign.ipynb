{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ef7e406-a24b-4bfb-bdaf-93473209a27a",
   "metadata": {},
   "source": [
    "### 1. Import and Install Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f907f38f-4000-4edf-9b7b-22d98f46eecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time \n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a86686-efd5-494c-9e9f-9b6706a05d0a",
   "metadata": {},
   "source": [
    "### 2. Keypoints using MP Holistic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ccd6893-f47e-4246-9b08-87d6ae247a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic # holistics model\n",
    "mp_drawing = mp.solutions.drawing_utils  # Drawing utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2331ea1-0535-4b17-929f-e6b8e32a8dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # convert image from BGR to RBG\n",
    "    image.flags.writeable = False # image is no longer  writable\n",
    "    result = model.process(image) # make prdictions\n",
    "    image.flags.writeable = True # image is now writable\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # convert image from RBG tto BGR\n",
    "    return image, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15bd33fe-b5f1-4efb-b47a-3f1a81938958",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS) # Draw Face connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)  # Draw Pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw LEFT hand connection\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw RIGHT hand connections\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf82bff8-b9ce-4668-824e-fb1b97d5a7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_style_landmarks(image, results):\n",
    "    # Draw Face connections\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, \n",
    "                              mp_holistic.FACEMESH_CONTOURS, \n",
    "                              mp_drawing.DrawingSpec(color=(80,110,10),thickness=1,circle_radius=1),\n",
    "                              mp_drawing.DrawingSpec(color=(80,256,121),thickness=1,circle_radius=1)\n",
    "                             ) \n",
    "    \n",
    "    # Draw Pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, \n",
    "                              mp_holistic.POSE_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(80,22,10),thickness=2,circle_radius=4),\n",
    "                              mp_drawing.DrawingSpec(color=(80,44,121),thickness=2,circle_radius=2)\n",
    "                             )  \n",
    "    \n",
    "     # Draw LEFT hand connection\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, \n",
    "                              mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(121,22,76),thickness=2,circle_radius=4),\n",
    "                              mp_drawing.DrawingSpec(color=(121,44,250),thickness=2,circle_radius=2)\n",
    "                             )\n",
    "    \n",
    "    # Draw RIGHT hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, \n",
    "                              mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(245,117,66),thickness=2,circle_radius=4),\n",
    "                              mp_drawing.DrawingSpec(color=(245,66,230),thickness=1,circle_radius=1)\n",
    "                             ) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb4cd78-a3bb-4136-8043-8f71cbb183bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic: \n",
    "    while cap.isOpened():\n",
    "        \n",
    "        # read frames of each immge video\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Make predictions\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        \n",
    "        \n",
    "        # draw landmarks on video frame\n",
    "        draw_style_landmarks(image, results)\n",
    "        \n",
    "        # show the frame to the user\n",
    "        cv2.imshow('Sign Language feeds', image)\n",
    "\n",
    "        # beak from the frame or quit the shown frame to the user\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break   \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ec5d66-1f16-4d1f-b4d4-53f5e21f84d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_style_landmarks(image, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5eaa0f1-58ca-440d-9407-a79261e317f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610ae2bf-3d39-4f6a-a8ce-4beba9e13d24",
   "metadata": {},
   "source": [
    "### 3. Extract Keypoint Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0f610c-050a-488d-8949-b50ddb91a6e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmarks]).flatten() if results.pose_landmarks.landmarks else np.zeros(132)\n",
    "lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmarks]).flatten() if results.left_hand_landmarks.landmarks else np.zeros(21*3)\n",
    "rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmarks]).flatten() if results.right_hand_landmarks.landmarks else np.zeros(21*3)\n",
    "face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmarks]).flatten() if results.face_landmarks.landmarks else np.zeros(1404) \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895cfe51-3c65-40b8-9e9e-67f8756649c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c6090790-ae75-4273-a0b5-7146dffeb886",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros(21*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b70a36fb-3406-4513-a018-6946ef016a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmarks]).flatten() if results.pose_landmarks.landmarks else np.zeros(132)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmarks]).flatten() if results.left_hand_landmarks.landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmarks]).flatten() if results.right_hand_landmarks.landmarks else np.zeros(21*3)\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmarks]).flatten() if results.face_landmarks.landmarks else np.zeros(1404) \n",
    "    return np.concatenate(pose, face, lh, rh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2929bfc1-7d0d-49c8-9f0f-883d5ea7ca43",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4. Setup Folders for Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "87e6588b-cb3c-4d51-82cf-f973d1e73de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to get exported data numpy array\n",
    "DATA_PATH = os.path.join('MP_DATA')\n",
    "\n",
    "# list of actions to be etecte as our label\n",
    "actions = np.array(['hello','thanks', 'iloveyou'])\n",
    "\n",
    "# thirty videos worth of data\n",
    "no_sequence = 30\n",
    "\n",
    "# video has a length of thirty frames\n",
    "sequence_length = 30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "708f98b6-6a1a-4215-a336-c63f3dad8602",
   "metadata": {},
   "outputs": [],
   "source": [
    "for action in actions:\n",
    "    for sequence in range(no_sequence):\n",
    "        try:\n",
    "            os.makedirs(os.path.join(DATA_PATH, action, str(sequence)))\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7043a6-f622-4c86-ad14-02140a68ef06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "33010397-11fd-42cc-ace3-2df6e45fc346",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 5. Collect Keypoints Value for Training and Testing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24b237b-233a-4c9b-bc71-78e37e11c116",
   "metadata": {},
   "source": [
    "### 6. Process Data and Create labels and features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2da9ce-65cc-4e3e-9094-a81e0257e2d9",
   "metadata": {},
   "source": [
    "### 7. Build and Train LSTM Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc87b7ea-4651-4a40-8865-4ecd62f051a0",
   "metadata": {},
   "source": [
    "### 8. Make Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a61ccf-e545-4baa-89a9-f7b00b2e865e",
   "metadata": {},
   "source": [
    "### 9. Save Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e2e140-68b0-4bd1-804b-cd7f86286ada",
   "metadata": {},
   "source": [
    "### 10. Evalute using Confusion Matrix and Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92218a6c-7b36-49e3-a783-fa373fd7d2e2",
   "metadata": {},
   "source": [
    "### 11. Text in Real Time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
